{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auteurs : Hachemin Pierre-Yves et Seys Thibaut\n",
    "\n",
    "Date : 25/04/2018\n",
    "\n",
    "Cours : IS3024AB - Data Mining\n",
    "\n",
    "# Etude sur le transport en Ile-de-France\n",
    "\n",
    "* [Introduction](#Introduction)\n",
    "\n",
    "    + [Contexte et objectifs](#Contexte-et-objectifs)\n",
    "    \n",
    "    + [Requirements](#Requirements)\n",
    "\n",
    "* [Datasets](#Datasets)\n",
    "\n",
    "    + [Description des datasets](#Description-des-datasets)\n",
    "    \n",
    "    + [Composition des datatsets](#Composition-des-datasets)\n",
    "    \n",
    "* [Traitements des datasets](#Traitements-des-datasets)\n",
    "\n",
    "    + [Datasets de validation](#Datasets-de-validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### Contexte et objectifs\n",
    "\n",
    "Pour notre projet de Data Mining, nous avons choisi d'explorer les données issues de la palteforme Open Data du Syndicat des Transports d'Ile-de-France (STIF) disponible à [cette adresse](https://opendata.stif.info/). Nous avons décidé d'explorer les données issues de la validation des voyageurs sur les différents réseaux ferrés d'Ile-de-France.\n",
    "\n",
    "Notre objectif est de mener une double étude sur les caractéristiques actuelles du réseau ferré et de son efficacité. Nous allons donc d'une part appliquer des algorithmes de clustering sur les différentes stations du réseau afin de mettre en évidence les profils types. Nous allons d'autre part essayer de recréer des lignes de transports reliant les différentes stations et comparer les résultats obtenus au réseau actuellement en place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "Pour que notre projet fonctionne correctement, nous avons besoin d'importer les libraries suivantes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import folium\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "### Description des datasets\n",
    "\n",
    "Parmis les jeux de données mis à disposition par la plateforme Open Data du STIF, nous avons choisi d'utiliser les 4 types de datasets suivants :\n",
    "\n",
    "- [validations-nombre-par-jour-2017s1](https://opendata.stif.info/explore/dataset/validations-sur-le-reseau-ferre-nombre-de-validations-par-jour-1er-sem/) et [validations-nombre-par-jour-2017s2](https://opendata.stif.info/explore/dataset/validations-sur-le-reseau-ferre-profils-horaires-par-jour-type-2e-sem/) : contiennent le nombre de validations par jour par station par catégorie de titre de transport pour chaqeu semestre de 2017.\n",
    "\n",
    "- [validations-profils-horaires-2017s1](https://opendata.stif.info/explore/dataset/validations-sur-le-reseau-ferre-profils-horaires-par-jour-type-1er-sem/) et [validations-profils-horaires-2017s2](https://opendata.stif.info/explore/dataset/validations-sur-le-reseau-ferre-profils-horaires-par-jour-type-2e-sem/) : contiennent les pourcentages moyens de validation par tranche horaire à la station donnée suivant le type de journée (jour ordinaire, dimanche, etc...)  pour chaque semestre de 2017.\n",
    "\n",
    "- [emplacement-des-gares-idf](https://opendata.stif.info/explore/dataset/emplacement-des-gares-idf/) : contient les emplacements de l'ensemble des stations du réseau ferré d'Ile-de-France.\n",
    "\n",
    "- [referentiel-arret-tc-idf-](https://opendata.stif.info/explore/dataset/referentiel-arret-tc-idf/) : recense l'ensemble des stations des réseaux de surface et ferré de transport d'Ile-de-France suivant différents niveaux de granularités.\n",
    "\n",
    "Tous ces datasets sont disponibles dans le dossier `Data` sous forme de fichiers csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composition des datasets\n",
    "\n",
    "Nous allons décrire dans cette partie les différents champs que nous allons utiliser pour chaque type de datasets que nous utilisons. De la documentation plus précise émise par le STIF se trouve dans le dossier `doc` sous forme de fichiers PDF.\n",
    "\n",
    "#### **Datasets du nombre de validations par jour**\n",
    "\n",
    "Les champs que nous avons choisi de garder sont représentés dans le tableau suivant :\n",
    "\n",
    "| Nom du champs | Description | Valeur |\n",
    "|---------------|-------------|--------|\n",
    "| LIBELLE_ARRET | Libellée de l'arrêt | string |\n",
    "| ID_REFA_LDA | Id de l'arrêt | float |\n",
    "| JOUR | Jour de l'année | string |\n",
    "| CATEGORIE_TITRE | Catégorie du titre | AMETHYSTE, AUTRE TITRE, FGT, IMAGINE R, NAVIGO, TST |\n",
    "| NB_VALD | Nombre de validations | int ou 'Moins de 5' |\n",
    "\n",
    "#### **Datasets du profil de validation**\n",
    "\n",
    "Les champs que nous avons choisi de garder sont représentés dans le tableau suivant :\n",
    "\n",
    "| Nom du champs | Description | Valeur |\n",
    "|---------------|-------------|--------|\n",
    "| LIBELLE_ARRET | Libellée de l'arrêt | string |\n",
    "| ID_REFA_LDA | Id de l'arrêt | float |\n",
    "| CAT_JOUR | Type de journée | JOHV, SAHV, JOVS, SAVS, DIJFP |\n",
    "| TRNC_HORR_60 | Tranche horaire | Interval (ex: 13H - 14H) |\n",
    "| pourc_validations | Pourcentage de validations | float |\n",
    "\n",
    "#### **Datasets du référentiels des stations du réseau**\n",
    "\n",
    "La modélisation d'un arrêt est une chose compliquée et le STIF possède trois niveaux de détails décrit ci-après du niveau le plus large au moins large :\n",
    "\n",
    "1. LDA : le premier niveau concerne l'ensemble des arrêts du même nom. Par exemple, pour l'arrêt 'République' il y a à la fois les différentes lignes de métro et les arrêts de bus.\n",
    "\n",
    "2. ZDL : le second niveau décrit l'ensemble des arrêts d'un même mode de transport. En reprenant l'exemple de l'arrêt 'République', on aura une ZDL pour le métro et une autre pour les arrêts de bus.\n",
    "\n",
    "3. ZDE : le dernier niveau décrit une zone d'embarquement. C'est l'endroit où l'usager peut physiquement emprunter la ligne de transport. Dans le cas de la ZDL des métro à 'République', il y aura une ZDE par ligne de métro.\n",
    "\n",
    "Ce dataset nous permet de faire les liens entre les stations des datasets de validation qui contiennet l'id de la LDA et le dataset des emplacements des stations qui contiennent les id de ZDE et ZDL. Voici les champs que nous allons utiliser au cours de notre traitement de données :\n",
    "\n",
    "| Nom du champs | Description | Valeur |\n",
    "|---------------|-------------|--------|\n",
    "| ZDEr_LIBELLE_TYPE_ARRET | Libellée du type d'arrêt | Arrêt de bus, Station ferrée / Val, Arrêt de tram, Station de métro, Station de funiculaire |\n",
    "| LDA_ID_REF_A | Id de la LDA | float |\n",
    "| ZDLr_ID_REF_A | Id de la ZDL | float |\n",
    "| ZDEr_ID_REF_A | Id de la ZDE | float |\n",
    "\n",
    "#### **Datasets des emplacements des stations du réseau ferré**\n",
    "\n",
    "Ce dataset va nous permettre de relier les données des datasets de validations à des positions géographiques et aux lignes du réseau ferré passant par ces stations. Les champs dont nous allons avoir besoin sont décrits dans le tableau suivant :\n",
    "\n",
    "| Nom du champs | Description | Valeur |\n",
    "|---------------|-------------|--------|\n",
    "| Geo Point | Coordonnées de la station | tuple |\n",
    "| LIGNES | Ligne passant par cette station | string |\n",
    "| ID_REF_ZDL | Id de la ZDL | float |\n",
    "| ID_REF_ZDE | Id de la ZDE | float |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traitements des datasets\n",
    "\n",
    "L'objectif de cette partie est d'arriver à un unique dataset sur lequel on effectuera nos calculs. Pour cela nous allons partir du dataset du nombre de validation par jour par station et le transformer pour avoir comme colonne : \n",
    "\n",
    "- JOUR : date du jour\n",
    "\n",
    "- LIBELLE_ARRET : Libellée de l'arrêt\n",
    "\n",
    "- ID_REFA_LDA : ID LDA de l'arrêt\n",
    "\n",
    "- LIGNES : Lignes passant par cette station\n",
    "\n",
    "- CAT_JOUR : Catégorie du jour parmi JOHV, SAHV, JOVS, SAVS et DIJFP\n",
    "\n",
    "- COORDINATE_X : Longitude de la station\n",
    "\n",
    "- COORDINATE_Y : Latitude de la station\n",
    "\n",
    "- TRANCHE_HORRAIRE (x24) : Une colonne par pourcentage de validation de la tranche horaire\n",
    "\n",
    "- NB_VALIDATION (x7) : Une colonne par titre de transport contenant le nombre de validation\n",
    "\n",
    "On arrivera donc à 38 colonnes par station par jour de l'année 2017.\n",
    "\n",
    "### Datasets de validation\n",
    "\n",
    "Pour commencer nous allons charger les datasets du nombre de valdiation et des profiles horaires. Pour les datasets de profils horaires, le STIF nous fournit une ligne par arrêt par catégorie de jour par tranche horraire. Or, nous souhaitons obtenir une ligne par arrêt et par catégorie de jour, les pourcentages suivants la tranche horaire étant contenus dans une colonne par tranche. Pour le dataset du nombre de validation par jour, nous avons exactement le même problème avec les catégories de titres de transport. Nous allons donc effectuer cette transformation au moment où nous chargeons les données dans des DataFrames pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction générique permettant le chargement des csv de validations\n",
    "def load_csv(csv_filename, columns, to_flatten, value):\n",
    "    with open(csv_filename, 'r', newline='\\n') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=';')\n",
    "        \n",
    "        data = {}\n",
    "        header = {elt: index for index, elt in enumerate(next(csv_reader))}\n",
    "        flattens = set()\n",
    "        \n",
    "        for row in csv_reader:\n",
    "            key = tuple(row[header[column]] for column in columns)\n",
    "            flattens.add(row[header[to_flatten]])\n",
    "            try:\n",
    "                data[key][row[header[to_flatten]]] = row[header[value]]\n",
    "            except KeyError:\n",
    "                data[key] = {row[header[to_flatten]]: row[header[value]]}\n",
    "                \n",
    "        df_dict = {elt:  [] for elt in columns}\n",
    "        df_dict.update({elt: [] for elt in flattens})\n",
    "    \n",
    "        for key, values in data.items():\n",
    "            for index, column in enumerate(columns):\n",
    "                df_dict[column].append(key[index])\n",
    "            for flatten in flattens:\n",
    "                df_dict[flatten].append(values.get(flatten, 0)) \n",
    "        \n",
    "        return pd.DataFrame(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des différentes DataFrame\n",
    "p_columns = ['LIBELLE_ARRET', 'CAT_JOUR', 'ID_REFA_LDA']\n",
    "p_flatten = 'TRNC_HORR_60'\n",
    "p_value = 'pourc_validations'\n",
    "\n",
    "v_columns = ['LIBELLE_ARRET', 'JOUR', 'ID_REFA_LDA']\n",
    "v_flatten = 'CATEGORIE_TITRE'\n",
    "v_value = 'NB_VALD'\n",
    "\n",
    "profile_s1 = load_csv('Data/validations-profils-horaires-2017s1.csv', p_columns, p_flatten, p_value)\n",
    "profile_s2 = load_csv('Data/validations-profils-horaires-2017s2.csv', p_columns, p_flatten, p_value)\n",
    "\n",
    "validation_s1 = load_csv('Data/validations-nombre-par-jour-2017s1.csv', v_columns, v_flatten, v_value)\n",
    "validation_s2 = load_csv('Data/validations-nombre-par-jour-2017s2.csv', v_columns, v_flatten, v_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons ensuite finir le chargement de ces datasets en réalisant une suite de pré-traitements comme supprimer les colonnes 'NON DEFINI' et les lignes contenant un ID_REFA_LDA nul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_s1 = profile_s1.drop('ND', axis=1)\n",
    "profile_s1 = profile_s1.drop(profile_s1[profile_s1['ID_REFA_LDA'] == ''].index)\n",
    "\n",
    "profile_s2 = profile_s2.drop('ND', axis=1)\n",
    "profile_s2 = profile_s2.drop(profile_s2[profile_s2['ID_REFA_LDA'] == ''].index)\n",
    "\n",
    "validation_s1 = validation_s1.drop('NON DEFINI', axis=1)\n",
    "validation_s1 = validation_s1.drop(validation_s1[validation_s1['ID_REFA_LDA'] == ''].index)\n",
    "\n",
    "validation_s2 = validation_s2.drop('NON DEFINI', axis=1)\n",
    "validation_s2 = validation_s2.drop(validation_s2[validation_s2['ID_REFA_LDA'] == ''].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emplacement des stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctions utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dac-project",
   "language": "python",
   "name": "dac-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
